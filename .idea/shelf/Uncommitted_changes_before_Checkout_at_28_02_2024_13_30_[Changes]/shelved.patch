Index: SAC/main_sac.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\r\nimport argparse\r\n\r\nimport gym\r\nimport numpy as np\r\nimport pandas\r\nfrom sac_torch import Agent\r\nfrom utils import plot_learning_curve\r\nfrom gym import wrappers\r\n\r\nimport environment.utils.fidgrovePluginUtils as utils\r\nfrom environment.rFactor2Environment import RFactor2Environment\r\n\r\n\r\ndef parse_args():\r\n    parser = argparse.ArgumentParser(description='PyTorch Soft Actor-Critic Args')\r\n    parser.add_argument('--env-name', default='RFactor2-v0',\r\n                        help='Mujoco Gym environment (default: RFactor2-v0)')\r\n    parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\r\n                        help='discount factor for reward (default: 0.99)')\r\n    parser.add_argument('--tau', type=float, default=0.005, metavar='G',\r\n                        help='target smoothing coefficient(τ) (default: 0.005)')\r\n    parser.add_argument('--beta', type=float, default=0.001, metavar='G',\r\n                        help='learning rate (default: 0.001)')\r\n    parser.add_argument('--alpha', type=float, default=0.2, metavar='G',\r\n                        help='Temperature parameter α determines the relative importance of the entropy\\\r\n                                term against the reward (default: 0.2)')\r\n    parser.add_argument('--start_steps', type=int, default=10000, metavar='N',\r\n                        help='number of start steps (default: 40000)')\r\n    parser.add_argument('--n_episodes', type=int, default=250, metavar='N',\r\n                        help='number of episodes (default: 250)')\r\n    parser.add_argument('--batch_size', type=int, default=256, metavar='N',\r\n                        help='batch size (default: 256)')\r\n    parser.add_argument('--hidden_size', type=int, default=256, metavar='N',\r\n                        help='hidden size (default: 256)')\r\n    parser.add_argument('--replay_size', type=int, default=1000000, metavar='N',\r\n                        help='size of replay buffer (default: 10000000)')\r\n    parser.add_argument('--epsilon', type=float, default=0.10, help='epsilon for epsilon greedy (default: 0.10')\r\n    parser.add_argument('--input_file', default=os.path.abspath('environment/common/inputs.csv'), help='file name with\\\r\n                                                                                         initial inputs')\r\n    parser.add_argument('--states_file', default=os.path.abspath('environment/common/transitions.csv'), help='file name with\\\r\n                                                                            state transitions of initial inputs')\r\n    parser.add_argument('--skip_initial', type=bool, default=False, help='skip initial transitions training')\r\n    parser.add_argument('--training_file', default=os.path.abspath('environment/common/initial_training.pkl'),\r\n                        help='file name with training inputs')\r\n    parser.add_argument('--coefficients', nargs=3, type=float, default=None, help='to be used coefficients')\r\n\r\n    return parser.parse_args()\r\n\r\n\r\nif __name__ == '__main__':\r\n    args = parse_args()\r\n\r\n    print(f\"Creating Environment {args.env_name}\")\r\n    # env = gym.make(args.env_name)\r\n    env = RFactor2Environment()\r\n\r\n    agent = Agent(alpha=args.alpha, gamma=args.gamma, tau=args.tau, beta=args.beta,\r\n                  input_dims=env.observation_space.shape[0],\r\n                  env=env, n_actions=env.action_space.shape[0],\r\n                  layer1_size=args.hidden_size, layer2_size=args.hidden_size,\r\n                  batch_size=args.batch_size)\r\n    n_episodes = args.n_episodes\r\n\r\n    filename = 'inverted_pendulum.png'\r\n    figure_file = os.path.join(os.path.abspath('SAC\\\\plots'), filename)\r\n    print(figure_file)\r\n\r\n    best_score = env.reward_range[0]\r\n    score_history = []\r\n    load_checkpoint = False\r\n\r\n    if args.skip_initial:\r\n        print(\"Performing initial start steps\")\r\n\r\n        steps = 0\r\n\r\n        while steps < args.start_steps:\r\n            print(\"Current step: \", steps)\r\n            observation = env.reset()[0]\r\n            done = False\r\n\r\n            while not done:\r\n                action = env.action_space.sample()\r\n                observation_, reward, done, _, _ = env.step(action)\r\n\r\n                agent.remember(observation, action, reward, observation_, done)\r\n\r\n                if not load_checkpoint:\r\n                    agent.learn()\r\n\r\n                observation = observation_\r\n\r\n                steps += 1\r\n\r\n        # updates = 0\r\n        # agent.load_models()\r\n        # states_df = pandas.read_csv(args.states_file)\r\n        # utils.plot(states_df['Previous State'].apply(lambda y: y.strip('[]').split(',')), agent)\r\n    else:\r\n        states_df = pandas.read_csv(args.states_file)\r\n        updates = utils.process_transitions(pandas.read_csv(args.input_file, header=1),\r\n                                            states_df,\r\n                                            agent)\r\n        agent.save_models()\r\n        # utils.save_initial(args.training_file, agent)\r\n        utils.plot(states_df['Previous State'].apply(lambda y: y.strip('[]').split(',')), agent)\r\n\r\n    for i in range(n_episodes):\r\n        observation = env.reset()[0]\r\n        done = False\r\n        score = 0\r\n        while not done:\r\n            action = agent.choose_action(observation)\r\n            observation_, reward, done, _, _ = env.step(action)\r\n            score += reward\r\n\r\n            agent.remember(observation, action, reward, observation_, done)\r\n\r\n            if not load_checkpoint:\r\n                agent.learn()\r\n\r\n            observation = observation_\r\n\r\n        score_history.append(score)\r\n        avg_score = np.mean(score_history[-100:])\r\n\r\n        if avg_score > best_score:\r\n            best_score = avg_score\r\n            if not load_checkpoint:\r\n                agent.save_models()\r\n\r\n        print('episode ', i, 'score %.1f' % score, 'avg_score %.1f' % avg_score)\r\n\r\n    if not load_checkpoint:\r\n        x = [i + 1 for i in range(n_episodes)]\r\n        plot_learning_curve(x, score_history, figure_file, n_episodes)\r\n
===================================================================
diff --git a/SAC/main_sac.py b/SAC/main_sac.py
--- a/SAC/main_sac.py	
+++ b/SAC/main_sac.py	
@@ -1,3 +1,4 @@
+import itertools
 import os
 import argparse
 
@@ -6,6 +7,7 @@
 import pandas
 from sac_torch import Agent
 from utils import plot_learning_curve
+from utils import deepcopy
 from gym import wrappers
 
 import environment.utils.fidgrovePluginUtils as utils
@@ -47,6 +49,26 @@
 
     return parser.parse_args()
 
+def spinning_up():
+    args = parse_args()
+
+    print(f"Creating Environment {args.env_name}")
+    env = RFactor2Environment()
+
+    agent = Agent(alpha=args.alpha, gamma=args.gamma, tau=args.tau, beta=args.beta,
+                  input_dims=env.observation_space.shape[0],
+                  env=env, n_actions=env.action_space.shape[0],
+                  layer1_size=args.hidden_size, layer2_size=args.hidden_size,
+                  batch_size=args.batch_size)
+    agent_targ = deepcopy(agent)
+
+    # Freeze target networks with respect to optimizers (only update via polyak averaging)
+    for p in agent_targ.parameters():
+        p.requires_grad = False
+
+    n_episodes = args.n_episodes
+
+
 
 if __name__ == '__main__':
     args = parse_args()
Index: SAC/sac_torch.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import torch as T\r\nimport torch.nn.functional as F\r\nfrom buffer import ReplayBuffer\r\nfrom networks import ActorNetwork, CriticNetwork, ValueNetwork\r\nimport numpy\r\n\r\n\r\nclass Agent:\r\n\r\n    def __init__(self, alpha=0.0003, beta=0.0003, input_dims=4,\r\n                 env=None, gamma=0.99, n_actions=2, max_size=1000000, tau=0.005,\r\n                 layer1_size=256, layer2_size=256, batch_size=256):\r\n        self.gamma = gamma\r\n        self.tau = tau\r\n        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\r\n        self.batch_size = batch_size\r\n        self.n_actions = n_actions\r\n\r\n        self.actor = ActorNetwork(alpha, beta, input_dims, n_actions=n_actions,\r\n                                  name='actor', max_action=env.action_space.high)\r\n        self.critic_1 = CriticNetwork(beta, input_dims, n_actions=n_actions,\r\n                                      name='critic_1')\r\n        self.critic_2 = CriticNetwork(beta, input_dims, n_actions=n_actions,\r\n                                      name='critic_2')\r\n        self.value = ValueNetwork(beta, input_dims, name='value')\r\n        self.target_value = ValueNetwork(beta, input_dims, name='target_value')\r\n        self.value_mean = []\r\n\r\n        self.update_network_parameters(tau=1)\r\n\r\n        print(f\"Input dims: {input_dims}\")\r\n\r\n    def choose_action(self, observation):\r\n        state = T.Tensor(numpy.array(observation)).to(self.actor.device)\r\n        actions, _ = self.actor.sample_normal(state, reparameterize=True)\r\n\r\n        return actions.cpu().detach().numpy()[0]\r\n\r\n    def remember(self, state, action, reward, new_state, done):\r\n        self.memory.store_transition(state, action, reward, new_state, done)\r\n\r\n    def update_network_parameters(self, tau=None):\r\n        if tau is None:\r\n            tau = self.tau\r\n\r\n        target_value_params = self.target_value.named_parameters()\r\n        value_params = self.value.named_parameters()\r\n\r\n        target_value_state_dict = dict(target_value_params)\r\n        value_state_dict = dict(value_params)\r\n\r\n        for name in value_state_dict:\r\n            value_state_dict[name] = tau * value_state_dict[name].clone() + \\\r\n                                     (1 - tau) * target_value_state_dict[name].clone()\r\n\r\n        self.target_value.load_state_dict(value_state_dict)\r\n\r\n    def learn(self):\r\n        if self.memory.mem_cntr < self.batch_size:\r\n            return\r\n\r\n        state, action, reward, new_state, done = \\\r\n            self.memory.sample_buffer(self.batch_size)\r\n\r\n        reward = T.tensor(reward, dtype=T.float32).to(self.actor.device)\r\n        done = T.tensor(done).to(self.actor.device)\r\n        state_ = T.tensor(new_state, dtype=T.float32).to(self.actor.device)\r\n        state = T.tensor(state, dtype=T.float32).to(self.actor.device)\r\n        action = T.tensor(action, dtype=T.float32).to(self.actor.device)\r\n\r\n        value = self.value(state).view(-1)\r\n        value_ = self.target_value(state_).view(-1)\r\n        # print(f\"Value_: {value_}\\n\")\r\n        value_[done] = 0.0\r\n\r\n        self.value_mean.append(T.mean(value_).item())\r\n\r\n        # Value network loss\r\n        actions, log_probs = self.actor.sample_normal(state, reparameterize=False)\r\n        log_probs = log_probs.view(-1)\r\n\r\n        critic_value = self.obtain_critic_value(state, actions)\r\n\r\n        self.value.optimizer.zero_grad()\r\n        value_target = critic_value - log_probs\r\n        value_loss = 0.5 * F.mse_loss(value, value_target)\r\n        value_loss.backward(retain_graph=True)\r\n        self.value.optimizer.step()\r\n\r\n        # Critic network loss\r\n        self.critic_1.optimizer.zero_grad()\r\n        self.critic_2.optimizer.zero_grad()\r\n\r\n        q1_old_policy = self.critic_1.forward(state, action).view(-1)\r\n        q2_old_policy = self.critic_2.forward(state, action).view(-1)\r\n\r\n        q_hat = reward + self.gamma * value_\r\n        # From spinningup: q_hat = reward + self.gamma * (1 - done) *\r\n        #       (target_critic_value - self.actor.alpha * actions_log_prob_next_state)\r\n\r\n        # Spinningup doesn't do this 0.5, but here they are averaged out. Good??\r\n        critic_1_loss = 0.5 * F.mse_loss(q1_old_policy, q_hat)\r\n        critic_2_loss = 0.5 * F.mse_loss(q2_old_policy, q_hat)\r\n\r\n        critic_loss = critic_1_loss + critic_2_loss\r\n        critic_loss.backward()\r\n        self.critic_1.optimizer.step()\r\n        self.critic_2.optimizer.step()\r\n\r\n        # Actor network loss\r\n        self.actor.optimizer.zero_grad()\r\n\r\n        actions, log_probs = self.actor.sample_normal(state, reparameterize=True)\r\n        log_probs = log_probs.view(-1)\r\n\r\n        critic_value = self.obtain_critic_value(state, actions)\r\n\r\n        actor_loss = log_probs * self.actor.alpha - critic_value\r\n        actor_loss = T.mean(actor_loss)\r\n        actor_loss.backward()\r\n        self.actor.optimizer.step()\r\n\r\n        self.update_network_parameters()\r\n\r\n    def obtain_critic_value(self, state, actions):\r\n        q1_new_policy = self.critic_1.forward(state, actions)\r\n        q2_new_policy = self.critic_2.forward(state, actions)\r\n        critic_value = T.min(q1_new_policy, q2_new_policy)\r\n        critic_value = critic_value.view(-1)\r\n\r\n        return critic_value\r\n\r\n    def save_models(self):\r\n        print('.... saving models ....')\r\n        self.actor.save_checkpoint()\r\n        self.value.save_checkpoint()\r\n        self.target_value.save_checkpoint()\r\n        self.critic_1.save_checkpoint()\r\n        self.critic_2.save_checkpoint()\r\n\r\n    def load_models(self):\r\n        print('.... loading models ....')\r\n        self.actor.load_checkpoint()\r\n        self.value.load_checkpoint()\r\n        self.target_value.load_checkpoint()\r\n        self.critic_1.load_checkpoint()\r\n        self.critic_2.load_checkpoint()\r\n
===================================================================
diff --git a/SAC/sac_torch.py b/SAC/sac_torch.py
--- a/SAC/sac_torch.py	
+++ b/SAC/sac_torch.py	
@@ -1,3 +1,5 @@
+import itertools
+
 import torch as T
 import torch.nn.functional as F
 from buffer import ReplayBuffer
@@ -22,6 +24,8 @@
                                       name='critic_1')
         self.critic_2 = CriticNetwork(beta, input_dims, n_actions=n_actions,
                                       name='critic_2')
+        self.q_params = itertools.chain(self.critic_1.parameters(), self.critic_2.parameters())
+
         self.value = ValueNetwork(beta, input_dims, name='value')
         self.target_value = ValueNetwork(beta, input_dims, name='target_value')
         self.value_mean = []
