Index: fidgrovePluginUtils.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import math\r\nimport mmap\r\nimport os.path\r\nimport pickle\r\nimport time\r\n\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport win32event\r\n\r\n# rFactor2 plugin Setup\r\ntelemetryH = win32event.OpenEvent(win32event.EVENT_ALL_ACCESS, 0, \"WriteEventCarData\")\r\ntelemetryMMfile = mmap.mmap(-1, length=80, tagname=\"MyFileMappingCarData\", access=mmap.ACCESS_READ)\r\n\r\nvehicleScoringH = win32event.OpenEvent(win32event.EVENT_ALL_ACCESS, 0, \"WriteVehicleScoring\")\r\nvehicleScoringMMfile = mmap.mmap(-1, length=20, tagname=\"MyFileVehicleScoring\", access=mmap.ACCESS_READ)\r\n\r\n# CONSTANTS\r\nACTION_TIMEOUT_LIMIT = 100\r\nCO_PL, CO_DIST, CO_DONE = 1.0, 1.7, 0.75  # Reward Coefficients default values\r\nPATH = 'D:\\\\IST\\\\Tese\\\\Thesis-Project'\r\n\r\n# Normalization values\r\nwith open(os.path.join(PATH, 'common/scale_factors.pkl'), 'rb') as file:\r\n    scaling_factors = pickle.load(file)\r\n    min_values = pickle.load(file)\r\n\r\n\r\ndef load_coefficients(coefficients):\r\n    global CO_PL, CO_DIST, CO_DONE\r\n\r\n    CO_PL, CO_DIST, CO_DONE = coefficients\r\n\r\n\r\ndef load_initial(file_path):\r\n    print(\"Loading training file\")\r\n    with open(file_path, 'rb') as filename:\r\n        agent = pickle.load(filename)\r\n\r\n    return agent\r\n\r\n\r\ndef save_initial(file_path, agent):\r\n    print(\"Saving training file\")\r\n    with open(file_path, 'wb') as filename:\r\n        pickle.dump(agent, filename)\r\n\r\n\r\ndef plot(previous_states_df, agent):\r\n    print(\"Plotting...\")\r\n    num_samples = len(previous_states_df)\r\n\r\n    selected_indices = np.random.choice(len(np.array(previous_states_df)), num_samples, replace=False)\r\n    state_samples = np.array([np.array(previous_states_df)[i] for i in selected_indices], dtype=float)\r\n\r\n    # Initialize arrays to store the action values for each state\r\n    actions_steering = np.zeros(num_samples)\r\n    actions_throttle = np.zeros(num_samples)\r\n\r\n    # Calculate the actions for each state based on your policy\r\n    for i in range(num_samples):\r\n        action = agent.choose_action(state_samples[i])\r\n\r\n        actions_steering[i] = action[0]\r\n        actions_throttle[i] = action[1]\r\n\r\n    dist_state_samples = [el[5] for el in state_samples]\r\n\r\n    # Create the action heatmap\r\n    fig, (ax1, ax2) = plt.subplots(1, 2)\r\n    # Plot data on the first subplot\r\n    ax1.scatter(dist_state_samples, actions_steering)\r\n    ax1.set_title('Steering Actions')\r\n    ax1.set_xlabel('Lap Distance')\r\n    ax1.set_ylabel('Steering')\r\n\r\n    # Plot data on the second subplot\r\n    ax2.scatter(dist_state_samples, actions_throttle)\r\n    ax2.set_title('Throttle Actions')\r\n    ax2.set_xlabel('Distance')\r\n    ax2.set_ylabel('Throttle')\r\n\r\n    # Display both subplots using a single plt.show() call\r\n    plt.show()\r\n\r\n    with open(os.path.join(PATH, 'common/lists.pkl'), 'wb') as filename:\r\n        pickle.dump(state_samples, filename)\r\n        pickle.dump(actions_steering, filename)\r\n        pickle.dump(actions_throttle, filename)\r\n\r\n\r\ndef process_transitions(actions_df, states_df, agent):\r\n    print(\"Processing initial transitions\")\r\n    timer = time.process_time()\r\n    actions = []\r\n    prev_states = []\r\n    next_states = []\r\n    updates = 0\r\n\r\n    previous_states_df = states_df['Previous State'].apply(lambda x: x.strip('[]').split(','))\r\n    new_states_df = states_df['New State'].apply(lambda x: x.strip('[]').split(','))\r\n\r\n    for index, action in actions_df.iterrows():\r\n        action = np.array(action)\r\n\r\n        prev_state = np.array(previous_states_df[index], dtype=float)\r\n        new_state = np.array(new_states_df[index], dtype=float)\r\n\r\n        actions.append(action)\r\n        prev_states.append(prev_state)\r\n        next_states.append(new_state)\r\n\r\n        done = episode_finish(prev_state, new_state)\r\n        reward = calculate_reward(prev_state, new_state, done)\r\n\r\n        agent.remember(prev_state, action, reward, new_state, done)\r\n\r\n        # Update parameters of all the networks\r\n        agent.learn()\r\n        updates += 1\r\n\r\n    elapsed_time = time.process_time() - timer\r\n    print(f\"Initial inputs and parameter updates finished after {elapsed_time} seconds.\")\r\n\r\n    return updates\r\n\r\n\r\ndef reset_events():\r\n    win32event.ResetEvent(telemetryH)\r\n    win32event.ResetEvent(vehicleScoringH)\r\n\r\n\r\ndef calculate_heading(x, z):\r\n    if x > 0 and z > 0:\r\n        # 1st quadrant\r\n        heading = math.asin(x)\r\n    elif x > 0 > z:\r\n        # 2nd quadrant\r\n        heading = math.pi - math.asin(x)\r\n    elif x < 0 and z < 0:\r\n        # 3rd quadrant\r\n        heading = math.pi - math.asin(x)\r\n    else:\r\n        # 4th quadrant\r\n        heading = 2 * math.pi + math.asin(x)\r\n\r\n    return heading\r\n\r\n\r\n# Calculated based on how much distance was advanced since last state and current velocity\r\ndef calculate_reward(prev_state, state, done):\r\n    lap_dist_prev = float(prev_state[5])\r\n    lap_dist_new = float(state[5])\r\n    # vel = float(state[3])\r\n    pl = float(state[6])\r\n\r\n    # Necessary because of resetting and plugin interaction\r\n    if abs(lap_dist_new - lap_dist_prev) > 500:\r\n        return 0\r\n    elif done:\r\n        penalty = 1 / (lap_dist_new * scaling_factors[5])\r\n        print(\"Penalty: \", penalty)\r\n    else:\r\n        penalty = 0\r\n\r\n    # print(f\"Lap dist diff: {lap_dist_new - lap_dist_prev}\\t\r\n    # With coefficient: {c_dist * (lap_dist_new - lap_dist_prev)}\")\r\n    # print(f\"Path lateral: {abs(pl)}\\tWith Coefficient: {c_pl * abs(pl)}\")\r\n\r\n    reward = CO_DIST * (lap_dist_new - lap_dist_prev) - \\\r\n             CO_PL * abs(pl) - \\\r\n             CO_DONE * penalty\r\n\r\n    return reward\r\n\r\n\r\n# Checks if a lap is completed or if the agent goes backwards (with some margin and reset care)\r\n# Or if it drives out of bounds or if it times out\r\ncount = 0\r\ntimeout_dist = 0\r\n\r\n\r\ndef episode_finish(prev_state, state):\r\n    timeout = False\r\n    global count\r\n    global timeout_dist\r\n    lap_dist_prev = float(prev_state[5])\r\n    lap_dist_new = float(state[5])\r\n    pl = float(state[6])\r\n\r\n    if count == 0:\r\n        timeout_dist = lap_dist_new\r\n\r\n    # print(f\"Difference: {lap_dist_prev - lap_dist_new}\\tPath Lateral: {pl}\")\r\n    if count % ACTION_TIMEOUT_LIMIT == 0 and count > 0:\r\n        timeout = abs(timeout_dist - lap_dist_new) < 30\r\n        timeout_dist = lap_dist_new\r\n\r\n    count += 1\r\n\r\n    cond_pl = abs(pl) >= 8.0\r\n    cond_start_pl = lap_dist_new < 200 and pl > 5.5\r\n\r\n    done = (cond_pl or cond_start_pl or timeout) and count < 20\r\n    if done:\r\n        print(\"PL: \", cond_pl)\r\n        print(\"Start: \", cond_start_pl)\r\n        print(\"Timeout: \", timeout)\r\n        count = 0\r\n\r\n    return done\r\n\r\n\r\ndef scale_features(state):\r\n    scaled_state = [(state[i] - min_value_i) * scale_factor_i - 1.0 for i, scale_factor_i, min_value_i in\r\n                    zip(range(len(state)), scaling_factors, min_values)]\r\n\r\n    return np.array(scaled_state)\r\n\r\n\r\ndef obtain_state():\r\n    win32event.WaitForMultipleObjects([vehicleScoringH, telemetryH], True, win32event.INFINITE)\r\n\r\n    telemetry_data = telemetryMMfile.read().decode(\"utf-8\").rstrip('\\x00').replace(\"\\n\", \"\").split(',')\r\n    telemetryMMfile.seek(0)\r\n\r\n    vehicle_data = vehicleScoringMMfile.read().decode(\"utf-8\").rstrip('\\x00').replace(\"\\n\", \"\").split(',')\r\n    vehicleScoringMMfile.seek(0)\r\n\r\n    # Position\r\n    position = [round(float(telemetry_data[0]), 2), round(float(telemetry_data[2]), 2)]\r\n    position_x, position_y = position\r\n    # Angle\r\n    orientation = [float(telemetry_data[3]), float(telemetry_data[5])]\r\n    heading = calculate_heading(orientation[0], orientation[1])\r\n    # Velocity\r\n    velocity = -float(telemetry_data[8])\r\n    # Acceleration\r\n    acceleration = -float(telemetry_data[11])\r\n    # Lap Distance\r\n    lap_dist = float(vehicle_data[0])\r\n    # Path Lateral - Distance to center of track\r\n    path_lateral = float(vehicle_data[1])\r\n\r\n    # Compile information into state variable (Maybe turn into class/dict)\r\n    state = np.array([position_x, position_y, heading, velocity, acceleration, lap_dist, path_lateral])\r\n\r\n    return state\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/fidgrovePluginUtils.py b/fidgrovePluginUtils.py
--- a/fidgrovePluginUtils.py	(revision 06998547406dacec0b27b82ef8c3101f53c83eab)
+++ b/fidgrovePluginUtils.py	(date 1701090596198)
@@ -181,7 +181,7 @@
 
 
 def episode_finish(prev_state, state):
-    timeout = False
+    cond_timeout = False
     global count
     global timeout_dist
     lap_dist_prev = float(prev_state[5])
@@ -191,21 +191,21 @@
     if count == 0:
         timeout_dist = lap_dist_new
 
+    count += 1
+
     # print(f"Difference: {lap_dist_prev - lap_dist_new}\tPath Lateral: {pl}")
-    if count % ACTION_TIMEOUT_LIMIT == 0 and count > 0:
-        timeout = abs(timeout_dist - lap_dist_new) < 30
+    if count % ACTION_TIMEOUT_LIMIT == 0:
+        cond_timeout = abs(timeout_dist - lap_dist_new) < 30
         timeout_dist = lap_dist_new
 
-    count += 1
-
     cond_pl = abs(pl) >= 8.0
     cond_start_pl = lap_dist_new < 200 and pl > 5.5
 
-    done = (cond_pl or cond_start_pl or timeout) and count < 20
+    done = (cond_pl or cond_start_pl or cond_timeout) and count > 20
     if done:
         print("PL: ", cond_pl)
         print("Start: ", cond_start_pl)
-        print("Timeout: ", timeout)
+        print("Timeout: ", cond_timeout)
         count = 0
 
     return done
Index: .idea/Thesis-Project.iml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<module type=\"PYTHON_MODULE\" version=\"4\">\r\n  <component name=\"NewModuleRootManager\">\r\n    <content url=\"file://$MODULE_DIR$\">\r\n      <sourceFolder url=\"file://$MODULE_DIR$/common\" isTestSource=\"false\" />\r\n      <excludeFolder url=\"file://$MODULE_DIR$/venv\" />\r\n    </content>\r\n    <orderEntry type=\"jdk\" jdkName=\"Python 3.10 (Thesis-Project)\" jdkType=\"Python SDK\" />\r\n    <orderEntry type=\"sourceFolder\" forTests=\"false\" />\r\n  </component>\r\n</module>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/Thesis-Project.iml b/.idea/Thesis-Project.iml
--- a/.idea/Thesis-Project.iml	(revision 06998547406dacec0b27b82ef8c3101f53c83eab)
+++ b/.idea/Thesis-Project.iml	(date 1701097194897)
@@ -2,7 +2,6 @@
 <module type="PYTHON_MODULE" version="4">
   <component name="NewModuleRootManager">
     <content url="file://$MODULE_DIR$">
-      <sourceFolder url="file://$MODULE_DIR$/common" isTestSource="false" />
       <excludeFolder url="file://$MODULE_DIR$/venv" />
     </content>
     <orderEntry type="jdk" jdkName="Python 3.10 (Thesis-Project)" jdkType="Python SDK" />
Index: SAC/main_sac.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\r\nimport argparse\r\n\r\nimport gym\r\nimport numpy as np\r\nimport pandas\r\nfrom sac_torch import Agent\r\nfrom utils import plot_learning_curve\r\nfrom gym import wrappers\r\n\r\nimport fidgrovePluginUtils as utils\r\nimport rFactor2Environment\r\n\r\n# CONSTANTS\r\nPATH = 'D:\\\\IST\\\\Tese\\\\Thesis-Project'\r\n\r\n\r\ndef parse_args():\r\n    parser = argparse.ArgumentParser(description='PyTorch Soft Actor-Critic Args')\r\n    parser.add_argument('--env-name', default='RFactor2-v0',\r\n                        help='Mujoco Gym environment (default: RFactor2-v0)')\r\n    parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\r\n                        help='discount factor for reward (default: 0.99)')\r\n    parser.add_argument('--tau', type=float, default=0.005, metavar='G',\r\n                        help='target smoothing coefficient(τ) (default: 0.005)')\r\n    parser.add_argument('--beta', type=float, default=0.0003, metavar='G',\r\n                        help='learning rate (default: 0.0003)')\r\n    parser.add_argument('--alpha', type=float, default=0.2, metavar='G',\r\n                        help='Temperature parameter α determines the relative importance of the entropy\\\r\n                                term against the reward (default: 0.2)')\r\n    parser.add_argument('--n_episodes', type=int, default=250, metavar='N',\r\n                        help='number of 250 (default: 250)')\r\n    parser.add_argument('--batch_size', type=int, default=256, metavar='N',\r\n                        help='batch size (default: 256)')\r\n    parser.add_argument('--hidden_size', type=int, default=256, metavar='N',\r\n                        help='hidden size (default: 256)')\r\n    parser.add_argument('--replay_size', type=int, default=1000000, metavar='N',\r\n                        help='size of replay buffer (default: 10000000)')\r\n    parser.add_argument('--epsilon', type=float, default=0.10, help='epsilon for epsilon greedy (default: 0.10')\r\n    parser.add_argument('--input_file', default=os.path.join(PATH, 'common/inputs.csv'), help='file name with\\\r\n                                                                                         initial inputs')\r\n    parser.add_argument('--states_file', default=os.path.join(PATH, 'common/transitions.csv'), help='file name with\\\r\n                                                                            state transitions of initial inputs')\r\n    parser.add_argument('--skip_initial', type=bool, default=False, help='skip initial transitions training')\r\n    parser.add_argument('--training_file', default=os.path.join(PATH, 'common/initial_training.pkl'),\r\n                        help='file name with training inputs')\r\n    parser.add_argument('--coefficients', nargs=3, type=float, default=None, help='to be used coefficients')\r\n\r\n    return parser.parse_args()\r\n\r\n\r\nif __name__ == '__main__':\r\n    args = parse_args()\r\n\r\n    print(f\"Creating Environment {args.env_name}\")\r\n    env = gym.make(args.env_name)\r\n\r\n    agent = Agent(alpha=args.alpha, gamma=args.gamma, tau=args.tau, beta=args.beta,\r\n                  input_dims=env.observation_space.shape,\r\n                  env=env, n_actions=env.action_space.shape[0],\r\n                  layer1_size=args.hidden_size, layer2_size=args.hidden_size,\r\n                  batch_size=args.batch_size)\r\n    n_episodes = args.n_episodes\r\n\r\n    # uncomment this line and do a mkdir models && mkdir video if you want to\r\n    # record video of the agent playing the game.\r\n    # env = wrappers.Monitor(env, 'models/video', video_callable=lambda episode_id: True, force=True)\r\n    filename = 'inverted_pendulum.png'\r\n    figure_file = 'plots/' + filename\r\n\r\n    best_score = env.reward_range[0]\r\n    score_history = []\r\n    load_checkpoint = False\r\n\r\n    if load_checkpoint:\r\n        agent.load_models()\r\n\r\n    if args.skip_initial:\r\n        updates = 0\r\n        agent = utils.load_initial(args.training_file)\r\n        states_df = pandas.read_csv(args.states_file)\r\n        print(states_df)\r\n        utils.plot(states_df['Previous State'].apply(lambda y: y.strip('[]').split(',')), agent)\r\n    else:\r\n        states_df = pandas.read_csv(args.states_file)\r\n        updates = utils.process_transitions(pandas.read_csv(args.input_file, header=1),\r\n                                            states_df,\r\n                                            agent)\r\n        utils.save_initial(args.training_file, agent)\r\n        utils.plot(states_df['Previous State'].apply(lambda y: y.strip('[]').split(',')), agent)\r\n\r\n    for i in range(n_episodes):\r\n        observation = env.reset()[0]\r\n        done = False\r\n        score = 0\r\n        while not done:\r\n            action = agent.choose_action(observation)\r\n\r\n            observation_, reward, done, _, _ = env.step(action)\r\n            score += reward\r\n\r\n            agent.remember(observation, action, reward, observation_, done)\r\n\r\n            if not load_checkpoint:\r\n                agent.learn()\r\n\r\n            observation = observation_\r\n\r\n        score_history.append(score)\r\n        avg_score = np.mean(score_history[-100:])\r\n\r\n        if avg_score > best_score:\r\n            best_score = avg_score\r\n            if not load_checkpoint:\r\n                agent.save_models()\r\n\r\n        print('episode ', i, 'score %.1f' % score, 'avg_score %.1f' % avg_score)\r\n\r\n    if not load_checkpoint:\r\n        x = [i + 1 for i in range(n_episodes)]\r\n        plot_learning_curve(x, score_history, figure_file)\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/SAC/main_sac.py b/SAC/main_sac.py
--- a/SAC/main_sac.py	(revision 06998547406dacec0b27b82ef8c3101f53c83eab)
+++ b/SAC/main_sac.py	(date 1701095503683)
@@ -72,7 +72,7 @@
     score_history = []
     load_checkpoint = False
 
-    if load_checkpoint:
+    if not args.skip_initial:
         agent.load_models()
 
     if args.skip_initial:
