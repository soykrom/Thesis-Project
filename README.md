# Thesis Project

Autonomous Racing has become a field of growing interest, being a domain in which accuracy, safety and speed are crucial, forcing all used methodologies to be tested in an intense, complex and dynamic environment. This has allowed for big developments in areas such as vehicle control, vehicle modelling, trajectory planning and behaviour tuning. This project proposes a new and unique approach that combines Model Predictive Control, a popular method of process control, with the reinforcement learning algorithm Soft Actor-Critic, well suited for optimization problems with continuous action spaces. Characteristic of Model Predictive Control, the controllers' effectiveness hinges on the accuracy of the mathematical model of the system, however, developing one such system would be computationally expensive. In this work, this is a challenge addressed by the use of a nominal vehicle model that is then improved by Gaussian Processes. Employing Gaussian Processes with Model Predictive Control will also allow for a robust controller that is able to account for uncertainties and maintain safe driving behaviour. Integrating a learning mechanism results in an overall optimization of the racing problem, as the agent will be capable to explore actions that could lead to more optimal trajectories, as well as adapt to unexpected scenarios, and opening the window to tweaking the drivers' behaviours as desired.

![Base Diagram](https://github.com/soykrom/Thesis-Project/assets/49161310/025bb530-b5b0-4bd0-88aa-8ed7aed94c71)
